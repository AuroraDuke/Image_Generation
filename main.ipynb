{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93f4ad51-3cf4-421b-965d-20ad9d656db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Kurulum\n",
    "#!pip install transformers accelerate -q\n",
    "#!pip install torchvision pandas tqdm -q\n",
    "\n",
    "# üìö K√ºt√ºphaneler\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9d7930-b0a6-4427-aef7-26ba88129ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def print_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        device_index = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(device_index)\n",
    "        total_memory = torch.cuda.get_device_properties(device_index).total_memory / (1024**2)  # MB\n",
    "        allocated = torch.cuda.memory_allocated(device_index) / (1024**2)  # MB\n",
    "        reserved = torch.cuda.memory_reserved(device_index) / (1024**2)  # MB\n",
    "\n",
    "        print(f\"üñ•Ô∏è  CUDA Device         : {device_name}\")\n",
    "        print(f\"üß†  Total GPU Memory    : {total_memory:.2f} MB\")\n",
    "        print(f\"üì¶  Allocated Memory    : {allocated:.2f} MB\")\n",
    "        print(f\"üï≥Ô∏è  Reserved (cached)   : {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da6966b-393e-49b3-8622-c33f7cc93e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  CUDA Device         : NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "üß†  Total GPU Memory    : 8187.50 MB\n",
      "üì¶  Allocated Memory    : 0.00 MB\n",
      "üï≥Ô∏è  Reserved (cached)   : 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed83b6df-9f99-42b5-9758-111c920ff0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Hardware\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a04ddc-f128-41e9-af1d-fa68bdc64682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Model \n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33bb59-977f-43ec-9a31-8aa0c25be0a9",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c140e47d-9f0c-4a61-bf48-d1484c9bc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_image_dir = \"train/train\"\n",
    "test_image_dir = \"test/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253d0ce2-1f97-4892-bb63-cce12716c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, image_folder):\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "        self.image_folder = image_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_name = str(row[\"image_id\"]) + \".jpg\"  # √ñrn: 0 ‚Üí 0.jpg\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        caption = row[\"caption\"]\n",
    "        \n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099198-409f-4b55-881f-e48dc883c1c5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757a5383-824d-4a72-8aa4-e042ad4fc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionDataset(train_df, processor, train_image_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e786b54-645d-4a3b-ace1-6c5d1fafdc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61670759-2fe6-4755-a189-30d2a47497a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The image features a comic-style panel depicti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Colorful postcard featuring \"Greetings from Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Two vending machines display a variety of drin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A man speaks at the eGovernment Conference 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A close-up of several silver coins stacked tog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21362</th>\n",
       "      <td>21362</td>\n",
       "      <td>A female athlete in a UC San Diego softball un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21363</th>\n",
       "      <td>21363</td>\n",
       "      <td>The image showcases well-manicured nails featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21364</th>\n",
       "      <td>21364</td>\n",
       "      <td>The image shows various stainless steel pots, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21365</th>\n",
       "      <td>21365</td>\n",
       "      <td>The image showcases several bottles of wine, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21366</th>\n",
       "      <td>21366</td>\n",
       "      <td>The image is a poster featuring a stylized tre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21367 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                            caption\n",
       "0             0  The image features a comic-style panel depicti...\n",
       "1             1  Colorful postcard featuring \"Greetings from Ch...\n",
       "2             2  Two vending machines display a variety of drin...\n",
       "3             3  A man speaks at the eGovernment Conference 201...\n",
       "4             4  A close-up of several silver coins stacked tog...\n",
       "...         ...                                                ...\n",
       "21362     21362  A female athlete in a UC San Diego softball un...\n",
       "21363     21363  The image showcases well-manicured nails featu...\n",
       "21364     21364  The image shows various stainless steel pots, ...\n",
       "21365     21365  The image showcases several bottles of wine, p...\n",
       "21366     21366  The image is a poster featuring a stylized tre...\n",
       "\n",
       "[21367 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008614f9-3f54-49d1-a78b-6f286cf256bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10684/10684 [55:39<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Epoch 1 ‚Äî Loss: 3.0897 ‚Äî Time: 3339.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = {\"epoch\": [], \"loss\": [], \"time\": []}\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        labels = input_ids.clone().detach()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f\"üìò Epoch {epoch+1} ‚Äî Loss: {avg_loss:.4f} ‚Äî Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"loss\"].append(avg_loss)\n",
    "    history[\"time\"].append(epoch_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c296e2-248f-42a7-b23c-b11fcb7efdb1",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbe1ed9-7e63-47b5-8014-f665289a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb914c9-5532-445e-a936-d99373146fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=30)\n",
    "    return processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67bbd51-0563-4222-84a9-3af18f609822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3771/3771 [31:51<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    image_id = str(row[\"image_id\"]) + \".jpg\" \n",
    "    image_path = os.path.join(test_image_dir, image_id)\n",
    "    caption = generate_caption(image_path)\n",
    "    results.append({\n",
    "        \"id\": idx + 1,\n",
    "        \"image_id\": image_id,\n",
    "        \"image_model_response\": caption\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "249181ab-fca1-4202-a966-590d18611df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ CSV Kaydƒ±\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df[[\"image_id\", \"image_model_response\"]].rename(columns={\"image_model_response\": \"caption\"}).to_csv(\"submission.csv\", index=False)\n",
    "submission_df.to_csv(\"caption_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b858440-9774-4084-bb54-797576b9b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "from numpy import cov, trace, iscomplexobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "791a488b-8185-462b-9be6-db72ce7ebe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import sqrtm\n",
    "from numpy import cov, trace, iscomplexobj\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0aca330-c21d-4721-bab5-9f3ef628552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_fgd(solution_embed: np.ndarray, submission_embed: np.ndarray) -> float:\n",
    "    fgd_list = []\n",
    "    for _idx, (sol_emb_sample, sub_emb_sample) in enumerate(zip(solution_embed, submission_embed)):\n",
    "        sol_emb_sample_rshaped, sub_emb_sample_rshaped = sol_emb_sample.reshape((1, 384)), sub_emb_sample.reshape((1, 384))\n",
    "        e1 = np.concatenate([sol_emb_sample_rshaped, sol_emb_sample_rshaped])\n",
    "        e2 = np.concatenate([sub_emb_sample_rshaped, sub_emb_sample_rshaped])\n",
    "\n",
    "        mu1, sigma1 = e1.mean(axis=0), cov(e1, rowvar=False)\n",
    "        mu2, sigma2 = e2.mean(axis=0), cov(e2, rowvar=False)\n",
    "\n",
    "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "        if iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "        fgd = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        fgd_list.append(fgd)\n",
    "\n",
    "        if _idx % 100 == 0:\n",
    "            print(f\"Processed {_idx} samples\", end=\"\\r\")\n",
    "\n",
    "    return float(np.mean(fgd_list))\n",
    "\n",
    "\n",
    "def evaluate_fgd(train_csv_path, submission_csv_path):\n",
    "    # Load CSVs\n",
    "    gt = pd.read_csv(train_csv_path)\n",
    "    pred = pd.read_csv(submission_csv_path)\n",
    "\n",
    "    # Sort by image_id for alignment\n",
    "    gt_sorted = gt.sort_values(\"image_id\").reset_index(drop=True)\n",
    "    pred_sorted = pred.sort_values(\"image_id\").reset_index(drop=True)\n",
    "\n",
    "    # Load GTE-small model\n",
    "    model = SentenceTransformer(\"thenlper/gte-small\", device=\"cpu\") \n",
    "\n",
    "    # Embed captions\n",
    "    gt_embed = model.encode(gt_sorted[\"caption\"].tolist(), convert_to_numpy=True, normalize_embeddings=True)\n",
    "    pred_embed = model.encode(pred_sorted[\"caption\"].tolist(), convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # Calculate FGD score\n",
    "    score = calculate_fgd(gt_embed, pred_embed)\n",
    "    print(f\"\\nüîç FGD Score: {score:.6f} (lower is better)\")\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f3f2d9-63e8-46a9-9fd8-ca909ae86e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3700 samples\n",
      "üîç FGD Score: 0.449373 (lower is better)\n",
      "Final FGD Score: 0.4493729586714745\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_fgd(\"train.csv\", \"submission.csv\")\n",
    "print(f\"Final FGD Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872bde3-2b23-461c-86c6-7ba41cf7df55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
